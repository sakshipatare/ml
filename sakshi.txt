import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline 

df=pd.read_csv("heart.csv")

df.head()

print(df.shape)

df.isnull().sum()

df.tail()

df.info()

df.describe()

df.fillna(246.002933)

df.dtypes['age']

df.isnull()

df.drop('target',axis='columns')

df.drop(4)

df.iloc[1:6,2:4]

df.loc[1:6]

df.drop_duplicates()

df.fillna(df.mean())

df['age'].mean()

df.drop_duplicates().sum()

df.drop_duplicates(subset=['ca'])

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns

sns.distplot(df['chol'],kde=False,bins=10,color='violet')

plt.figure(figsize=(8,6))
sns.countplot(x='age',hue='target',data=df,color='pink')
plt.show()

print(df.to_string())

df1 = df.fillna(df.median())
print(df1.to_string())

df1.drop_duplicates()

df = df1.astype({'trestbps':'int','chol':'int','oldpeak':'int','slope':'int'})
df

X = df.drop('target',axis = 'columns')
y = df["target"]

X

y

x_train,x_test, y_train, y_test = train_test_split(X,y,test_size = 0.25)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

Reg = LogisticRegression()

Reg.fit(x_train, y_train)

y_predict = Reg.predict(x_test)

y_predict.shape

print(accuracy_score(y_test, y_predict))

print(classification_report(y_test, y_predict))

print(classification_report(y_test, y_predict))

print(confusion_matrix(y_test, y_predict))

sns.heatmap(confusion_matrix(y_test, y_predict),annot = True)



ASSSSS 22222

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from sklearn import metrics

df=pd.read_csv("weight-height.csv")
df

x=df.iloc[:,1:2]
y=df.iloc[:,2]
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25)
Reg = LinearRegression()
Reg.fit(x_train,y_train)

Y_predict = Reg.predict(x_test)

Y_predict.shape

print(Reg.coef_)

print(Reg.intercept_)

plt.scatter(x_test,y_test,color='green')

print('Mean Square Error',metrics.mean_squared_error(y_test, Y_predict))

print('Mean Absolute Error',metrics.mean_absolute_error(y_test, Y_predict))

rsquare = Reg.score(x_train, y_train)
rsquare



ASSSS 3333333333

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

df=pd.read_csv("Admission_Predict.csv")
df


df.to_string()

df.isna().sum()

df.columns=df.columns.str.rstrip()
df.loc[df['Chance of Admit']>=0.8,'Chance of Admit']=1
df.loc[df['Chance of Admit']<=0.8,'Chance of Admit']=0

x=df.iloc[:,1:7]
y=df.iloc[:,8]
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25)
model=DecisionTreeClassifier(criterion='entropy')
model.fit(x_train,y_train)

Y_predict = model.predict(x_test)

print(accuracy_score(y_test, Y_predict)) 

print(classification_report(y_test,Y_predict))

print(confusion_matrix(y_test, Y_predict))

sns.heatmap(confusion_matrix(y_test, Y_predict),annot = True)

feature_names=df.iloc[0:8]
print(feature_names,end='')

target_name=[str(x) for x in model.classes_]
target_name

from sklearn.tree import plot_tree
fig=plt.figure(figsize=(50,30))

plot_tree(model)

plt.savefig("Decision_tree_visualization.png")


x=df.iloc[:,1:7]
x

from sklearn import tree

sf= StratifiedKFold(n_splits=5,shuffle=True,random_state=0)

depth=[1,2,3,4,5,6,7,8,9,10]

for d in depth:
    score=cross_val_score(tree.DecisionTreeClassifier(criterion='entropy',max_depth=d,random_state=0),x_train,y_train,cv=sf,scoring='accuracy')
    print('Average score for depth {} is : {}'.format(d,score.mean()))



ASSSSSSSSSS 444444444

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("mall.csv")

df

df.shape

df.info()

X=df.iloc[:,[3,4]].values

X

from sklearn.cluster import KMeans

wcss=[]

for i in range(1,11):
    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
    
#plt.plot(range(1,11),wcss)
#plt.title('elbow method')
#plt.xlabel('number of clusters')
#plt.ylabel('wcss')

kmeans=KMeans(n_clusters=5,init='k-means++',max_iter=300,random_state=42)
y_kmeans=kmeans.fit_predict(X)

y_kmeans

y_kmeans.shape

plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=200,c='red',label='cluster1')
plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=200,c='blue',label='cluster2')
plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=200,c='green',label='cluster3')
plt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=200,c='magenta',label='cluster4')
plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=200,c='pink',label='cluster5')
plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='centroid')
plt.title("kmeans clustering")
plt.xlabel("annual income")
plt.ylabel("spending score")
plt.legend()
plt.show()

import matplotlib.cm as cm
from sklearn.metrics import silhouette_samples,silhouette_score

range_n_clusters=[2,3,4,5,6]
for n_clusters in range_n_clusters:
    clusterer=KMeans(n_clusters=n_clusters,random_state=10)
    cluster_labels=clusterer.fit_predict(X)
    
    silhouette_avg=silhouette_score(X,cluster_labels)
    print("for n_clusters=",n_clusters,"the average silhouette_score is ",silhouette_avg)


import scipy.cluster.hierarchy as sch
plt.figure(figsize=(10,10))
dendrogram=sch.dendrogram(sch.linkage(X,method='single'))
plt.title('dendrogram')
plt.xlabel('customers')
plt.ylabel('euclidean distances')
plt.show()

from sklearn.cluster import AgglomerativeClustering
hc=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='complete')
y_hc=hc.fit_predict(X)

plt.figure(figsize=(8,8))
plt.scatter(X[y_hc==0,0],X[y_hc==0,1],s=100,c='red',label='careful customers')
plt.scatter(X[y_hc==1,0],X[y_hc==1,1],s=100,c='blue',label='standard customers')
plt.scatter(X[y_hc==2,0],X[y_hc==2,1],s=100,c='green',label='target customers')
plt.scatter(X[y_hc==3,0],X[y_hc==3,1],s=100,c='cyan',label='careless customers')
plt.scatter(X[y_hc==4,0],X[y_hc==4,1],s=100,c='magenta',label='sensible customers')
plt.title("clusters of customer using hierarchical clustering")
plt.xlabel("annual income")
plt.ylabel("spending score")
plt.legend()
plt.show()




ASSSSS 5555555555


import pandas as pd
from keras.models import Sequential
from keras.layers import Dense

df = pd.read_csv('pima-indians-diabetes.csv')

df

df.info()

X=df.iloc[:0:-1].values
X

y=df.iloc[:,8].values
y

model=Sequential()
model.add(Dense(12,input_dim=8,activation='relu'))
model.add(Dense(8,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(X,y,epochs=150,batch_size=10)
accuracy=model.evaluate(X,y)
print("accuracy: %.2f" % (accuracy*100))
from ann_visualizer.visualize import ann_viz;
ann_viz(model,title='my first neural network')



